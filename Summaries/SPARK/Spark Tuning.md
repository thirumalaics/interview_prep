[Part I](https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-1/)
- concurrent tasks execute the same operations
- each stage contains a sequence of transformations that can be completed without shuffling the full data
- [coalesce and repartition](https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce)
	- coalesce is an optimized version of repartition
	- allows avoiding data movement for decreasing number of RDD partitions
	- coalesce avoids a full shuffle
		- as per the given value to coalesce, spark keeps the data on these many number of partitions
		- moves the data off the extra nodes on to the nodes where data did not move
		- repartition internally calls same method as coalesce but with shuffle argument set to True
		- both are narrow transformation
		- repartition algo creates new partitions with data distributed evenly
		- whereas coalesce may result in partitions with different amounts of data
			- and remember unequal sized partitions are generally slower to work with than equal sized partitions
- at each stage boundary, data is written to disks by tasks in the parent stages and then fetched over the nw by tasks in the child stage
	- involves heavy disk and nw IO
- avoid GBK when performing an associative reductive operation
	- sum, count
	- gbk transfers the entire dataset across the nw, rbk will compute local sums for each key in each partition and shuffles
- when shuffles are better
	- when the data arrives in large unsplittable files
		- repartitioning can will allow operations that come after it to leverage more of the cluster's CPU

[PART II](https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-2/)
#### Tuning resource allocation
- every executor in an application has the same fixed number of cores and heap size
- --executor-cores or `spark.executor.cores`
	- determines max num of concurrent tasks an executor can run
- `spark.executor.memory`
	- impacts the amount of data Spark can cache, as well as the maximum sizes of the shuffle data structures
- `spark.executor.instances`
	- num of executors requested
	- we can avoid setting this by turning dynamic allocation `spark.dynamicAllocation.enabled`
	- dynamic allocations enables a Spark application to request executors when there is a backlog of pending tasks and frees up executors when idle
- YARN has configurations that constrain the amount of resources that spark can use
	- `yarn.nodemanager.resource.memory-mb` controls the max sum of memory used by ***containers(plural)*** on each node
	- `yarn.nodemanager.resource.cup-vcords` controls the maximum sum of cores used by the ***containers(plural)*** on each node
- full mem requested to YARN for each executor `spark.yarn.executor.memoryOverhead` + `spark.executor.memory`
- yarn may also increment the requested memory up a little
- running executors with too much mem often results in excessive garbage collection delays, good upper limit is 65 gb
	- total mem available is an important factor affecting GC performance
	- throughput is [inversely proportional](https://docs.oracle.com/en/java/javase/17/gctuning/factors-affecting-garbage-collection-performance.html) to the amount of mem available
- shuffle partitions
- cluster with 6 nodes, 16 cores and 64 GB mem/ node
	- `yarn.nodemanager.resource.memory-mb` = 63GB * 1024
	- `yarn.nodemanager.resource.cpu-vcores` = 15
	- we avoid allocating 100% resources to the containers themselves, because the node needs some resources to run the OS and Hadoop daemons
		- so a gig and a core for these system processes on every machine
- the instinctive approach would be:
	- --num-executors 6 --executor-cores 15 --executor-memory 63G
		- this means
			- 1 executor per available node
			- all cores available to each executor
			- all available memory allocated to the executor alone
		- what this configuration does not consider
			- 63